# Part 1: The Conceptual Study Guide

### 1. The Core Architecture: The "Internet of Agents"
The central theme across these notebooks is decoupling. Instead of building one massive, monolithic agent, the ADK encourages a **Microservices Architecture for AI**.

*   **Local vs. Distributed:** In early learning, sub-agents run in the same Python process. In this advanced stage, agents run on different servers (or even different companies' infrastructure) and communicate over HTTP.
*   **The A2A Protocol:** This is the "HTTP" of the agent world. It is a standardized JSON-based protocol that allows agents to exchange tasks and results. It makes agents **Language Agnostic** (a Python agent can call a Node.js agent) and **Location Agnostic**.
*   **Vertex AI Agent Engine:** This is the hosting platform (Container Orchestrator). It manages the "Hardware" (CPU/RAM) and "Lifecycle" (Auto-scaling to zero) of your agent so it isn't just living in a temporary notebook session.

### 2. Key Distinctions

| Feature | Local Sub-Agents | Remote A2A Agents | Deployed Agent Engine |
| :--- | :--- | :--- | :--- |
| **Location** | Same RAM/Process | Different Port/Server/URL | Google Cloud Infrastructure |
| **Connection** | Direct Python Object reference | HTTP Request via Proxy | gRPC / Cloud API |
| **Latency** | Near-zero | Network dependent | Network + Cold Start time |
| **Use Case** | Tightly coupled logic (e.g., math tool) | Third-party services (e.g., Weather Vendor) | Production User Interfaces |

### 3. Essential Terminology

*   **`to_a2a(agent, port)`**: A wrapper function that takes a standard ADK agent and spins up a FastAPI/Starlette server around it, exposing endpoints for the A2A protocol.
*   **Agent Card (`/.well-known/agent-card.json`)**: The "Contract." A JSON file automatically generated by the server. It tells the world: "I am Agent X, I have these Tools, and here is how you talk to me." It is the Swagger/OpenAPI spec for Agents.
*   **`RemoteA2aAgent`**: The "Client Proxy." This is a Python object you create in your *consumer* agent. It points to the URL of the *provider* agent. You treat it like a local object, but internally it translates calls into network requests.
*   **`.agent_engine_config.json`**: The infrastructure-as-code file. It dictates how much CPU/RAM the agent gets when deployed to Vertex AI.
*   **`agent_engines.get()`**: The Vertex AI SDK method to reconnect to an agent that is already running in the cloud.

---

# Part 2: Essential Code Patterns

These snippets are refactored for clarity and production usage.

### Pattern 1: The "Provider" Agent (Server Side)
This is how you create an agent intended to be consumed by others.

```python
from google.adk.agents import LlmAgent
from google.adk.models.google_llm import Gemini
from google.adk.a2a.utils.agent_to_a2a import to_a2a

# 1. Define Tools (The "Skills")
def check_inventory(sku: str) -> str:
    # In production, this hits a database
    return f"SKU {sku}: 50 units in stock."

# 2. Define the Agent
provider_agent = LlmAgent(
    model=Gemini(model="gemini-2.5-flash-lite"),
    name="inventory_manager",
    description="Manages stock levels and product lookups.", # Critical for the Agent Card
    instruction="You are an inventory system. Use tools to look up data.",
    tools=[check_inventory]
)

# 3. Expose via A2A (Returns a Starlette App)
# Note: In production, you run this with uvicorn directly
app = to_a2a(provider_agent, port=8001)
```

### Pattern 2: The "Consumer" Agent (Client Side)
This is how you consume the provider defined above.

```python
from google.adk.agents import LlmAgent, RemoteA2aAgent
from google.adk.models.google_llm import Gemini

# 1. Create the Proxy (The Client)
# This reads the 'agent-card.json' from the provider to learn its skills
inventory_service = RemoteA2aAgent(
    name="remote_inventory",
    description="External service for checking stock.",
    agent_card="http://localhost:8001/.well-known/agent-card.json"
)

# 2. Create the Main Agent
main_agent = LlmAgent(
    model=Gemini(model="gemini-2.5-flash-lite"),
    name="customer_support",
    instruction="Answer user questions. Check inventory if asked about stock.",
    sub_agents=[inventory_service] # Treat remote agent exactly like a local tool
)
```

### Pattern 3: Deployment Configuration (File Structure)
To deploy to Vertex AI Agent Engine, you cannot just run a script. You must structure your folder exactly like this:

**File: `.agent_engine_config.json`** (Scaling logic)
```json
{
    "min_instances": 0,  
    "max_instances": 2,
    "resource_limits": {
        "cpu": "1",
        "memory": "2Gi"
    }
}
```

**File: `requirements.txt`** (Dependencies)
```text
google-adk
opentelemetry-instrumentation-google-genai
# Add any specific libraries your tools need here
```

**CLI Command:**
```bash
adk deploy agent_engine \
    --project=my-gcp-project \
    --region=us-central1 \
    ./my_agent_directory \
    --agent_engine_config_file=./.agent_engine_config.json
```

---

# Part 3: The "Master Script"

This script simulates a full A2A architecture locally. It spins up a **Weather Service (Provider)** in a background thread and then runs a **Travel Planner (Consumer)** that discovers and queries the weather service over the network.

**Prerequisites:** `pip install google-adk[a2a] uvicorn requests` and set `GOOGLE_API_KEY` in your environment.

```python
import os
import time
import threading
import uvicorn
import asyncio
from google.adk.agents import LlmAgent, RemoteA2aAgent
from google.adk.models.google_llm import Gemini
from google.adk.a2a.utils.agent_to_a2a import to_a2a
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types

# --- CONFIGURATION ---
HOST = "127.0.0.1"
PORT = 8001
AGENT_CARD_URL = f"http://{HOST}:{PORT}/.well-known/agent-card.json"

# ==========================================
# COMPONENT 1: THE VENDOR (Provider Agent)
# ==========================================
def get_current_weather(city: str) -> str:
    """Mock tool to simulate external data fetching."""
    weather_db = {
        "london": "Rainy, 15¬∞C",
        "paris": "Cloudy, 18¬∞C",
        "tokyo": "Sunny, 25¬∞C"
    }
    return weather_db.get(city.lower(), "Unknown weather data")

def create_vendor_app():
    """Creates the A2A app for the Weather Vendor."""
    agent = LlmAgent(
        model=Gemini(model="gemini-1.5-flash"),
        name="weather_vendor",
        description="External vendor providing real-time weather data.",
        instruction="You are a weather API. Use the get_current_weather tool strictly.",
        tools=[get_current_weather]
    )
    # wraps the agent in a FastAPI/Starlette app
    return to_a2a(agent, port=PORT)

def run_server():
    """Runs the vendor agent in a background thread."""
    app = create_vendor_app()
    uvicorn.run(app, host=HOST, port=PORT, log_level="error")

# ==========================================
# COMPONENT 2: THE CONSUMER (Your Agent)
# ==========================================
async def run_consumer_workflow():
    print(f"üîÑ Attempting to connect to Vendor at {AGENT_CARD_URL}...")
    
    # Wait for server to spin up
    retries = 5
    while retries > 0:
        try:
            # We construct the RemoteA2aAgent. 
            # This creates a 'Proxy' that looks like a local agent but talks HTTP.
            remote_weather_agent = RemoteA2aAgent(
                name="weather_service",
                description="Remote weather provider.",
                agent_card=AGENT_CARD_URL
            )
            break
        except Exception:
            time.sleep(1)
            retries -= 1
            
    if retries == 0:
        print("‚ùå Failed to connect to vendor server.")
        return

    # Define the Main Agent using the Remote Agent as a sub-agent
    travel_agent = LlmAgent(
        model=Gemini(model="gemini-1.5-flash"),
        name="travel_planner",
        instruction="""
        You are a Travel Planner. 
        If the user asks about a trip, ALWAYS check the weather using the 'weather_service' sub-agent first.
        Then, recommend packing tips based on that weather.
        """,
        sub_agents=[remote_weather_agent] # <--- The A2A Magic happens here
    )

    # --- EXECUTION (Session Management) ---
    session_service = InMemorySessionService()
    session_id = f"session_{int(time.time())}"
    await session_service.create_session("travel_app", "user_1", session_id)

    runner = Runner(agent=travel_agent, app_name="travel_app", session_service=session_service)
    
    user_query = "I am planning a trip to Tokyo. What should I pack?"
    print(f"\nüë§ User: {user_query}")
    print("ü§ñ Agent Thinking (communicating with remote vendor)...")
    
    response_text = ""
    # run_async handles the conversation loop and tool calls automatically
    async for event in runner.run_async("user_1", session_id, types.Content(parts=[types.Part(text=user_query)])):
        if event.is_final_response() and event.content:
            response_text = event.content.parts[0].text

    print(f"\n‚úàÔ∏è  Travel Planner Response:\n{'-'*40}\n{response_text}\n{'-'*40}")

# ==========================================
# MAIN ENTRY POINT
# ==========================================
if __name__ == "__main__":
    # 1. Start the "Remote" Server in background
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    
    # 2. Give it a moment to boot
    time.sleep(3)
    
    # 3. Run the Consumer Client
    try:
        asyncio.run(run_consumer_workflow())
    except KeyboardInterrupt:
        print("\nStopping...")
    
    print("\n‚úÖ Simulation Complete. (Background server will terminate with script)")
```